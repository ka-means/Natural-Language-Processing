{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF99zSnD4V6o"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM5xDd9l4V6p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WOcFvN44V6r"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset originates from Twitter data collected via the Twitter API, with initial filtering based on hate speech terminology from Hatebase.org. The collection process identified tweets containing specific terms flagged by the community as hate speech, resulting in an initial pool of 85.4 million tweets from 33,458 Twitter accounts.\n",
        "\n",
        "A random subset from this initial collection underwent detailed manual review. CrowdFlower workers evaluated and labeled each tweet into one of three distinct categories: hate speech, offensive language that doesn't qualify as hate speech, and content that is neither offensive nor hateful.\n",
        "\n",
        "Dataset structure\n",
        "\n",
        "For this task, we focus on two key columns:\n",
        "* tweet: The sentence requiring evaluation.\n",
        "* label: The classification assigned to each tweet.\n",
        "\n",
        "The label variable uses the following encoding:\n",
        "* 0 - hate speech\n",
        "* 1 - offensive language\n",
        "* 2 - neither\n",
        "\n",
        "The dataset comes pre-split into train (df_train), validation (df_val), and test (df_test) sets."
      ],
      "metadata": {
        "id": "aqEHhg1QDHaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1: Word2Vec**"
      ],
      "metadata": {
        "id": "a165HtWs6XfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1.1 Creating the embedding matrix**\n",
        "Generate the embedding matrix using the corpus from the dataset."
      ],
      "metadata": {
        "id": "DwU7atQ-rFgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary for tokenization later\n",
        "nltk.download('punkt')\n",
        "\n",
        "def load_datasets():\n",
        "    \"\"\"\n",
        "    Loads the train, validation, and test dataframes from the local environment.\n",
        "    \"\"\"\n",
        "    # Using the specific filenames from your Colab environment\n",
        "    train_path = 'df_train.csv'\n",
        "    val_path = 'df_val.csv'\n",
        "    test_path = 'df_test.csv'\n",
        "\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_val = pd.read_csv(val_path)\n",
        "    df_test = pd.read_csv(test_path)\n",
        "\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "def explore_data(df, name=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    Prints basic exploration statistics and class distribution.\n",
        "    \"\"\"\n",
        "    print(f\"--- Exploration: {name} ---\")\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "    print(\"\\nClass Distribution (normalized):\")\n",
        "    print(df['label'].value_counts(normalize=True))\n",
        "    print(\"\\nFirst 3 rows:\")\n",
        "    print(df[['tweet', 'label']].head(3))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Execution\n",
        "df_train, df_val, df_test = load_datasets()\n",
        "explore_data(df_train, \"Train Set\")"
      ],
      "metadata": {
        "id": "MUAj4Jkd7ZJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac42b5b-95f5-4e37-e969-d82321b0b93f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe a significant class imbalance: class 1 (Offensive Language) accounts for 77% of the data, while Hate Speech (class 0) represents only 5.7%. This is critical for Part 1, as the Word2Vec model will be predominantly exposed to terms from class 1.\n",
        "\n",
        "The tweets contain typical noise: mentions (@user), URLs, and special characters (like \ud83d\ude02 representing emojis). Without cleaning this, Word2Vec will treat HTML emoji codes as words, which will pollute our embedding matrix."
      ],
      "metadata": {
        "id": "7Hyz8yznjwTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Corpus for Word2Vec**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fQ408Xi2k26g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "    \"\"\"\n",
        "    Cleans tweet text for Word2Vec training.\n",
        "    Removes HTML entities, URLs, mentions, and non-alphabetic characters.\n",
        "    \"\"\"\n",
        "    # Remove HTML entities (e.g., &#128514; or &amp;)\n",
        "    text = re.sub(r'&\\w+;\\d+;', '', text)\n",
        "    text = re.sub(r'&\\w+;', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove user mentions (@user)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Keep only letters (remove numbers and special symbols)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize and lowercase\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "print(\"Starting preprocessing...\")\n",
        "df_train['tokens'] = df_train['tweet'].apply(preprocess_tweet)\n",
        "\n",
        "\n",
        "print(\"Preprocessing complete. Example of tokens:\")\n",
        "print(df_train[['tweet', 'tokens']].head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft6dNtV-krYQ",
        "outputId": "1ff5e337-e8f1-4e9b-a230-63d795a7822f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word2Vec Training + Matrix**"
      ],
      "metadata": {
        "id": "UmqpNq0ClkoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtMYGKw9oFsB",
        "outputId": "873b702e-9076-4d25-ff3a-069aba35b239"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def train_and_create_matrix(tokenized_series, vector_size=100):\n",
        "    \"\"\"\n",
        "    Trains Word2Vec and extracts the weights into a matrix.\n",
        "    \"\"\"\n",
        "    print(\"Training Word2Vec model...\")\n",
        "    # sentences: Our tokenized tweets\n",
        "    # vector_size: Dimensionality of the word vectors\n",
        "    # window: Maximum distance between the current and predicted word\n",
        "    # min_count: Ignores words with total frequency lower than 2\n",
        "    w2v_model = Word2Vec(\n",
        "        sentences=tokenized_series,\n",
        "        vector_size=vector_size,\n",
        "        window=5,\n",
        "        min_count=2,\n",
        "        workers=4\n",
        "    )\n",
        "\n",
        "    # Get the list of words in the vocabulary\n",
        "    words = w2v_model.wv.index_to_key\n",
        "    vocab_size = len(words)\n",
        "\n",
        "    # Initialize the matrix with zeros (index 0 is for padding)\n",
        "    embedding_matrix = np.zeros((vocab_size + 1, vector_size))\n",
        "\n",
        "    # Map words to indices (starting from 1)\n",
        "    word_to_idx = {word: i + 1 for i, word in enumerate(words)}\n",
        "\n",
        "    # Fill the matrix with the learned vectors\n",
        "    for word, i in word_to_idx.items():\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "    return w2v_model, embedding_matrix, word_to_idx\n",
        "\n",
        "# Execution\n",
        "w2v_model, embedding_matrix, word_to_idx = train_and_create_matrix(df_train['tokens'])\n",
        "\n",
        "print(f\"Vocabulary Size (excluding padding): {len(word_to_idx)}\")\n",
        "print(f\"Embedding Matrix Shape: {embedding_matrix.shape}\")\n",
        "\n",
        "# Quick verification: Words similar to 'bitch' (very frequent in this dataset)\n",
        "if 'bitches' in w2v_model.wv:\n",
        "    print(\"\\nSimilar words to 'bitches':\")\n",
        "    print(w2v_model.wv.most_similar('bitches', topn=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBvQUkjUlsM_",
        "outputId": "641b2a46-f098-46c1-f944-d2fe44bdb88c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What I did in the previous steps:**\n",
        "\n",
        "When training Word2Vec, we first needed to prepare the tweets. You can't just feed raw text to the algorithm because it would treat \"Talking\" and \"talking\" as different words, same with \"hoe!\" and \"hoe\". So tokenization breaks everything down into clean, lowercase words without punctuation.\n",
        "\n",
        "Once we fed this cleaned data to Word2Vec, the algorithm observed which words tend to appear near each other in the tweets. It noticed patterns. For example, it found that \"bitches\" and \"hoes\" appear in almost identical contexts in this dataset. That's why when we asked for similar words, it returned those with a score near 0.99, meaning they're used interchangeably.\n",
        "\n",
        "Now, the embedding matrix is where things get interesting. It has shape (8240, 100). The 8240 is our vocabulary size: every unique word the model learned plus one extra row. The 100 is the dimension, meaning each word is now represented by 100 numbers instead of just being text. These numbers capture abstract features the model discovered.\n",
        "\n",
        "That first row of zeros at index 0 is for padding. When a tweet is short, we fill the empty spaces with zeros so the model knows \"there's nothing here\" without it affecting the actual meaning of the text."
      ],
      "metadata": {
        "id": "Rclf9xOgoxz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 Train MLP classifier\n",
        "\n",
        "Train a classifier based on a Multi-Layer Perceptron (MLP) model, keeping the embedding matrix weights frozen. Plot the loss function and accuracy curves for both train and validation sets.\n",
        "\n",
        "Obtain detailed metrics for Precision, Recall, and F1-score on the test set (overall and per class). Comment on the results.\n",
        "\n",
        "Obtain a confusion matrix. Comment on it."
      ],
      "metadata": {
        "id": "s5BQfOnvq4VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def transform_to_padded_sequences(tokenized_series, word_to_idx, max_length=50):\n",
        "    sequences = []\n",
        "    for tokens in tokenized_series:\n",
        "        indices = [word_to_idx[word] for word in tokens if word in word_to_idx]\n",
        "        sequences.append(indices)\n",
        "    return pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# validation and test tokens exist?\n",
        "if 'tokens' not in df_val.columns:\n",
        "    df_val['tokens'] = df_val['tweet'].apply(preprocess_tweet)\n",
        "if 'tokens' not in df_test.columns:\n",
        "    df_test['tokens'] = df_test['tweet'].apply(preprocess_tweet)\n",
        "\n",
        "MAX_LEN = 50\n",
        "X_train = transform_to_padded_sequences(df_train['tokens'], word_to_idx, MAX_LEN)\n",
        "X_val   = transform_to_padded_sequences(df_val['tokens'], word_to_idx, MAX_LEN)\n",
        "X_test  = transform_to_padded_sequences(df_test['tokens'], word_to_idx, MAX_LEN)\n",
        "\n",
        "y_train = df_train['label'].values\n",
        "y_val   = df_val['label'].values\n",
        "y_test  = df_test['label'].values\n",
        "\n",
        "# MLP Model\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "mlp_model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_LEN,\n",
        "        trainable=False\n",
        "    ),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print(\"Training MLP...\")\n",
        "history = mlp_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax1.plot(history.history['loss'], label='Train')\n",
        "ax1.plot(history.history['val_loss'], label='Val')\n",
        "ax1.set_title('Loss'); ax1.legend()\n",
        "ax2.plot(history.history['accuracy'], label='Train')\n",
        "ax2.plot(history.history['val_accuracy'], label='Val')\n",
        "ax2.set_title('Accuracy'); ax2.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xaExs0-mIKjV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "279c550b-7a7b-4324-87a7-b2b4f5d5610a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def generate_evaluation_report(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Generates classification report and confusion matrix for the test set.\n",
        "    \"\"\"\n",
        "    # Get predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    target_names = ['Hate Speech (0)', 'Offensive (1)', 'Neither (2)']\n",
        "\n",
        "    # 1. Detailed Metrics\n",
        "    print(\"\\n--- DETAILED METRICS (TEST SET) ---\")\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "    # 2. Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names, yticklabels=target_names)\n",
        "    plt.title('Confusion Matrix - MLP (Fixed Word2Vec)')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "generate_evaluation_report(mlp_model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "bHMdYyWQtway",
        "outputId": "561feea9-1d88-4c09-d62c-b2e72c8b26b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What I discovered when analyzing the model results:**\n",
        "\n",
        "The most striking finding is that the model didn't detect a single case of Hate Speech. It literally classified all 153 hate examples as something else, probably as \"Offensive\". The Recall of 0.00 means it completely failed on this class.\n",
        "\n",
        "This happened for two main reasons. First, the dataset is heavily imbalanced: the hate class barely represents 5.7% of the data. The model found it easier to minimize error by completely ignoring this class and focusing on class 1, which gave it 81% accuracy with minimal effort. Second, the Word2Vec embeddings are frozen. Since we left trainable=False, the model can't adjust the vectors to better separate the classes. For Word2Vec, a racist slur and a generic insult share similar contexts, so their vectors are very close in semantic space. If we don't allow these vectors to adjust during training, the MLP doesn't have enough information to capture the subtle difference between hate and offensive language.\n",
        "\n",
        "Class 1 (Offensive) is where the model excelled, with an F1-score of 0.89. This makes sense because there are thousands of examples of this class and Word2Vec captures aggressive social media language well.\n",
        "\n",
        "Class 2 (Neither) had moderate performance with F1 of 0.58. The recall of 0.54 means almost half of the neutral tweets were confused with offensive language. Probably because many neutral tweets in this dataset use informal language that the model associates with aggression.\n",
        "\n",
        "Looking at the confusion matrix, we observe a large concentration in the class 1 column. The model learned that when in doubt, betting on \"Offensive\" is the safest option to keep the error low.\n",
        "\n",
        "The training curves showed us that Val_Loss remained stable between 0.43 and 0.49. There was no severe overfitting here, which makes sense: with frozen embeddings, the model has little freedom to memorize noise from the training set. But this same rigidity is what prevents it from learning to distinguish the hate class.\n",
        "\n",
        "This model with static Word2Vec is insufficient for detecting hate speech in imbalanced data. Although we achieved 81% accuracy, it doesn't serve what really matters to us: identifying the minority class. This justifies using more advanced techniques like SBERT, which captures the complete context of the sentence and allows better semantic separation."
      ],
      "metadata": {
        "id": "r0oKO5GNuvF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2: SentenceBERT**"
      ],
      "metadata": {
        "id": "WAmScZX66MEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1 Creating SBERT embeddings\n",
        "\n",
        "Using Sentence Transformers, obtain the vector representations for each comment in the train and test sets.\n",
        "\n",
        "More information at: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
        "\n",
        "**Note**\n",
        "\n",
        "* GPU must be used. Go to \"Runtime\" --> \"Change runtime type\" --> Select \"T4 GPU\".\n",
        "* It is recommended to save the created dataframes.\n",
        "* Do not consider the validation dataset."
      ],
      "metadata": {
        "id": "K0Se5RNBC3Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKh2fbaNoCYU",
        "outputId": "0f6444ff-820d-462b-e793-bd9ceba7584a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def generate_and_save_embeddings(df_train, df_test):\n",
        "    \"\"\"\n",
        "    Generates SBERT embeddings for train and test sets and saves them to disk.\n",
        "    \"\"\"\n",
        "    print(\"Generating embeddings for Train set...\")\n",
        "\n",
        "    train_embeddings = sbert_model.encode(df_train['tweet'].tolist(), show_progress_bar=True)\n",
        "\n",
        "    print(\"Generating embeddings for Test set...\")\n",
        "    test_embeddings = sbert_model.encode(df_test['tweet'].tolist(), show_progress_bar=True)\n",
        "\n",
        "\n",
        "    np.save('X_train_sbert.npy', train_embeddings)\n",
        "    np.save('X_test_sbert.npy', test_embeddings)\n",
        "\n",
        "    return train_embeddings, test_embeddings\n",
        "\n",
        "\n",
        "X_train_sbert, X_test_sbert = generate_and_save_embeddings(df_train, df_test)\n",
        "\n",
        "print(f\"\\nEmbeddings created:\")\n",
        "print(f\"X_train_sbert shape: {X_train_sbert.shape}\")\n",
        "print(f\"X_test_sbert shape:  {X_test_sbert.shape}\")"
      ],
      "metadata": {
        "id": "MSk0_xugHO_k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "e15c66be780e466aa03e1a21b95d4ac0",
            "edd34a5eec9f42c0b8fe405cecff36b9",
            "90133a8478e543b2bdf7e654b206b78b",
            "03ca3169d69d4b4f89bab6989ea826fe",
            "ccb7d38e222f4dd3ab64b9c115cfdfcf",
            "8636d1f89a4e41f4b88b75f651fe6da7",
            "12d553e9088a4780b0a13545d55678fc",
            "ecdfa839a9d24f56be93d2c51b8f2af0",
            "65099d043d6447e9bb14cc781bc45e52",
            "13261969a80a44b5ae762e25a2047f8b",
            "338cc70d464845e598252396b32d8839",
            "57b687f8ffdd41d2b831864dc4da6659",
            "934251043fc840eab661a53c8a01c71d",
            "dd03b213b4ce47b08cbcc640436d11e6",
            "2f470e5c0acb4cc9bf80684342b31733",
            "954e843aa55247229090a7453012308d",
            "861b454d470644f7b3992e7f917f6328",
            "4ba897a57aca4f72ab820417c6564dd9",
            "813d30b79e2c4f26b52b520742e98d9d",
            "91afdcd809114cda84ba5f82a22bb43b",
            "b74123824f9d49b9a083b5cec4622d0b",
            "cf78939f6b9e42a5a09790d2b0052d78"
          ]
        },
        "outputId": "bb5ca52e-09e1-40f5-b743-ce1730cff2da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **About the previous result:**\n",
        "\n",
        "Unlike Word2Vec embeddings (which had 100 dimensions based on words), these SBERT embeddings have 384 dimensions that represent the global semantic meaning of each tweet.\n",
        "\n",
        "This increase in dimensionality allows capturing much finer nuances of the complete sentence, including irony, grammatical structure, and overall sentiment. SBERT generates dynamic representations. This means that an \"aggressive\" word in a joking context will have a different vector than the same word in an actual attack context."
      ],
      "metadata": {
        "id": "dfatKJBS0Q5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 2.2 Visualization of SBERT embeddings**\n",
        "\n",
        "Using the embeddings created for the train set, apply a dimensionality reduction technique such as PCA, TSNE, or UMAP and plot your results in 2D or 3D. Mark each category of the label variable with a different color. Comment on your results."
      ],
      "metadata": {
        "id": "4bYnepHeJFOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_sbert_2d(embeddings, labels):\n",
        "    \"\"\"\n",
        "    Reduces SBERT embeddings to 2D using PCA and creates a scatter plot.\n",
        "    \"\"\"\n",
        "    # PCA to reduce from 384 to 2 dimensions\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "\n",
        "    label_map = {0: 'Hate Speech (0)', 1: 'Offensive (1)', 2: 'Neither (2)'}\n",
        "    label_names = [label_map[l] for l in labels]\n",
        "\n",
        "\n",
        "    sns.scatterplot(\n",
        "        x=embeddings_2d[:, 0],\n",
        "        y=embeddings_2d[:, 1],\n",
        "        hue=label_names,\n",
        "        palette={'Hate Speech (0)': '#e74c3c', 'Offensive (1)': '#3498db', 'Neither (2)': '#2ecc71'},\n",
        "        alpha=0.4,\n",
        "        s=25\n",
        "    )\n",
        "\n",
        "    plt.title('2D Visualization of SBERT Embeddings (PCA)')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.legend(title='Category', loc='upper right')\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_sbert_2d(X_train_sbert, y_train)"
      ],
      "metadata": {
        "id": "oJkBTfLCHRob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "523264be-9f37-4a26-fbab-8eebe1338246"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What the SBERT visualization reveals:**\n",
        "\n",
        "By reducing the 384 dimensions to 2 with PCA, the plot shows something interesting. The \"Neither\" class (green) forms its own clearly separated zone in the lower part. SBERT manages to identify that neutral tweets have a completely different structure and vocabulary from aggressive language.\n",
        "\n",
        "The real problem is with the hate class. The red points (Hate Speech) are completely mixed within the large blue mass (Offensive). There's no visible separation between them. This confirms what we already suspected: hate speech and offensive language occupy practically the same vector space because they share similar contexts.\n",
        "\n",
        "What I do notice is that SBERT creates more defined clusters compared to Word2Vec. Although classes 0 and 1 overlap, the points are more concentrated in certain regions, suggesting there are subtle patterns that a well-trained classifier could learn to capture.\n",
        "\n",
        "The imbalance is brutally clear in the plot. The blue cloud dominates the entire space, especially in the upper and central area. It's obvious why the model tends to classify everything as \"Offensive\": that class is everywhere and any simple model will bet on it to minimize error.\n",
        "\n",
        "SBERT does offer a clear advantage over Word2Vec: it separates neutral from aggressive well. But the distinction between hate and offense remains the real challenge, and this visualization makes it very clear. They're too close together in semantic space."
      ],
      "metadata": {
        "id": "RYsMXJus11-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.3 Train classifier\n",
        "\n",
        "Using the embeddings created in section 2.1, train a classifier using a machine learning technique.\n",
        "\n",
        "Calculate the Precision, Recall, and F1-score metrics for the test set (overall and per class). Comment on the results.\n",
        "\n",
        "Obtain a confusion matrix. Comment on it."
      ],
      "metadata": {
        "id": "0IAQwMECK8Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_and_evaluate_sbert_classifier(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Trains a Random Forest classifier using SBERT embeddings and evaluates its performance.\n",
        "    \"\"\"\n",
        "    print(\"Training Random Forest Classifier (this may take a moment)...\")\n",
        "\n",
        "    # We use class_weight='balanced' to handle the extreme class imbalance\n",
        "\n",
        "    rf_classifier = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # 1. Predictions\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    target_names = ['Hate Speech (0)', 'Offensive (1)', 'Neither (2)']\n",
        "\n",
        "    # 2. Detailed Metrics Report\n",
        "    print(\"\\n--- CLASSIFICATION REPORT (SBERT + Random Forest) ---\")\n",
        "    report = classification_report(y_test, y_pred, target_names=target_names)\n",
        "    print(report)\n",
        "\n",
        "    # 3. Confusion Matrix Visualization\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names, yticklabels=target_names)\n",
        "\n",
        "    plt.title('Confusion Matrix: SBERT + Random Forest')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "    return rf_classifier\n",
        "\n",
        "\n",
        "# Using the SBERT embeddings and original labels\n",
        "sbert_model = train_and_evaluate_sbert_classifier(X_train_sbert, y_train, X_test_sbert, y_test)"
      ],
      "metadata": {
        "id": "PA0TsgVYHcA_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "outputId": "c3ce6f13-2aae-4895-bd86-f146379f85d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **I chose Random Forest and obtained these results:**\n",
        "\n",
        "I chose Random Forest for three concrete reasons. First, it handles the 384 SBERT dimensions well without requiring prior scaling. Second, with class_weight='balanced' it automatically adjusts class weights according to their frequency, which is crucial when Hate Speech only represents 5.7% of the data. Third, it can capture complex non-linear relationships between embedding dimensions, something linear models cannot achieve.\n",
        "\n",
        "The accuracy remained around 0.80, but this metric is misleading due to the dominance of class 1. The Weighted F1-score of 0.74 better reflects the actual performance.\n",
        "\n",
        "For Hate Speech, precision was perfect (1.00) but recall was terrible (0.01). This means that when the model predicts hate, it's always right, but it only detects 1% of the actual cases. It's progress compared to Word2Vec which detected nothing, but it's still insufficient for a system that works in reality.\n",
        "\n",
        "Offensive Language has recall of 1.00 and F1-score of 0.89. This is the class where the model feels comfortable. Since it's the majority, when in doubt it classifies there, which is why it captures all real cases but precision drops to 0.79.\n",
        "\n",
        "Neither has high precision (0.95): when it says something is neutral, it generally is. But recall is low (0.24), meaning most neutral tweets are being classified as \"Offensive\".\n",
        "\n",
        "The confusion matrix clearly shows the problem. Almost all errors from classes 0 and 2 end up in the class 1 column. Although SBERT provides better representations than Word2Vec, the data imbalance (77% vs 5% and 17%) creates a kind of \"statistical gravity\" that pulls predictions toward the majority class."
      ],
      "metadata": {
        "id": "fGaPQpqc3yC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Additional Analysis**\n",
        "\n",
        "To deepen the analysis of what we achieved (and what we didn't) with SBERT and Random Forest, I'm going to use 4 additional visualizations that are industry standard for diagnosing classification models with class imbalance.\n",
        "\n",
        "1. Precision-Recall (PR) Curve per Class: (To see the trade-off between precision and recall for different decision thresholds.)\n",
        "2. Prediction Probability Histogram: (To see how \"confident\" the model is.)\n",
        "3. Feature Importance: (To identify whether the model is relying on just a few dimensions (low robustness) or if it's using the full richness of the embedding.)\n",
        "4. Error Analysis: Print the actual tweets where the model failed spectacularly."
      ],
      "metadata": {
        "id": "sjdqQiTx8B6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Precision-Recall Curve (Per class)**\n"
      ],
      "metadata": {
        "id": "5QAHgF769MB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Binarize labels for multi-class PR curve calculation\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "y_score = sbert_model.predict_proba(X_test_sbert)\n",
        "\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "class_names = ['Hate Speech (0)', 'Offensive (1)', 'Neither (2)']\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "for i in range(3):\n",
        "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    avg_prec = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
        "    plt.plot(recall, precision, color=colors[i], lw=2,\n",
        "             label=f'{class_names[i]} (Avg Precision: {avg_prec:.2f})')\n",
        "\n",
        "plt.xlabel('Recall (Sensitivity)')\n",
        "plt.ylabel('Precision (Exactness)')\n",
        "plt.title('Precision-Recall Curve per Category')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "EhpdmjV3-e2U",
        "outputId": "82fd8244-bee7-4dea-d7bf-e706afc57836"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What the Precision-Recall curves show:**\n",
        "\n",
        "This plot reveals how the model balances between not making mistakes (precision) and finding all real cases (recall) for each class.\n",
        "\n",
        "The blue curve (Offensive) is practically glued to the top and right. The model is excellent at detecting offensive language because it has thousands of examples and the patterns are clear. It maintains high precision even with high recall.\n",
        "\n",
        "The green curve (Neither) has a decent area under the curve. It stays above 0.80 precision until reaching approximately 0.60 recall, then starts to drop. The model knows how to identify neutral messages, but when there are informal or ambiguous words it starts to hesitate and confuses them with the offensive class.\n",
        "\n",
        "The red curve (Hate Speech) is the disaster. It starts at 1.0 precision but with recall practically at zero. As you try to increase recall to detect more hate cases, precision plummets brutally. Around 0.30 recall it's already below 0.40 precision. This shows the model has a very low ceiling: if you adjust it to be more sensitive to hate, it starts classifying everything as hate and precision crashes. It's the classic problem of the minority class in imbalanced data.\n",
        "\n",
        "The area under the curve for Hate Speech (0.32) compared to Offensive (0.97) and Neither (0.83) makes clear where the model's real problem lies."
      ],
      "metadata": {
        "id": "EWZCubs1__kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Probability Histogram**"
      ],
      "metadata": {
        "id": "K5tL9tBp-_XL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract the probability assigned to the predicted class\n",
        "max_probabilities = np.max(y_score, axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(max_probabilities, bins=30, kde=True, color='teal')\n",
        "plt.axvline(np.mean(max_probabilities), color='red', linestyle='--',\n",
        "            label=f'Mean Confidence: {np.mean(max_probabilities):.2f}')\n",
        "plt.title('Distribution of Model Confidence in Predictions')\n",
        "plt.xlabel('Maximum Probability Assigned')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "n0qWJnbc_FuF",
        "outputId": "76527d44-9893-433d-f483-f0da611618f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What the confidence distribution reveals:**\n",
        "\n",
        "This histogram shows how confident the model is when making its predictions. The mean confidence is 0.80, marked with the red dashed line.\n",
        "\n",
        "The distribution is clearly right-skewed. There's a huge concentration of predictions between 0.85 and 0.95, with the highest peak around 0.92. This means the model is quite \"arrogant\": it's very confident in its decisions most of the time.\n",
        "\n",
        "The problem is that this high confidence doesn't necessarily mean it's correct. As we saw in the previous metrics, the model fails often with minority classes, but does so with high confidence. This confirms it has learned a simple pattern: when in doubt, it assigns high probability to the Offensive class because it's the most frequent in training.\n",
        "\n",
        "There are relatively few cases in the low confidence range (0.4-0.6), suggesting the model rarely truly \"hesitates\". In an ideal scenario with balanced classes, we would expect to see more predictions with moderate confidence at class boundaries. But here the model has learned to be decisive, even though that decision almost always favors the majority class.\n",
        "\n",
        "This distribution explains why Hate Speech recall is so low: the model would need very high confidence to predict hate, and it simply doesn't reach that threshold because it learned that betting on Offensive is statistically safer."
      ],
      "metadata": {
        "id": "Ybz9F9Q_AyjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Dimension Importance (Top 20)**"
      ],
      "metadata": {
        "id": "NOZDMjfk_J4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from the Random Forest model\n",
        "importances = sbert_model.feature_importances_\n",
        "top_indices = np.argsort(importances)[-20:]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title('Top 20 SBERT Dimensions by Importance')\n",
        "plt.barh(range(len(top_indices)), importances[top_indices], color='darkblue', align='center')\n",
        "plt.yticks(range(len(top_indices)), [f'Dimension {i}' for i in top_indices])\n",
        "plt.xlabel('Relative Importance Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "xRLcNc9r_MdC",
        "outputId": "aa074417-9699-4065-9fc3-35a377b6a688"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What the most important SBERT dimensions reveal:**\n",
        "\n",
        "SBERT represents each tweet as a vector of 384 numbers. This plot shows which of those dimensions were most useful for Random Forest to make decisions.\n",
        "\n",
        "Dimensions 103, 279, and 106 are the most important, with values around 0.015. They're followed by dimension 69 at approximately 0.013, then dimension 288 at around 0.011. From there, importance gradually decreases.\n",
        "\n",
        "What's interesting is that there's no dominant dimension that shoots far above the rest. The difference between the most important (103) and number 20 (356) isn't dramatic. This indicates the model is using information distributed across multiple dimensions to make decisions, which is a good sign.\n",
        "\n",
        "If there were one giant bar and the rest tiny, it would mean the model is clinging to a single pattern or feature, which would be simplistic and not robust. Instead, this more uniform distribution suggests Random Forest is capturing a more complex and holistic view of the tweet's semantic content.\n",
        "\n",
        "We can't know exactly what each dimension represents (SBERT is a black box in that sense), but the fact that multiple dimensions contribute significantly confirms the model is leveraging the richness of the contextual representation that SBERT provides."
      ],
      "metadata": {
        "id": "LLMs4xh3BBlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Error Analysis (Tweet Inspection)**"
      ],
      "metadata": {
        "id": "BWa1kz_v_MxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Generate final predictions\n",
        "y_test_preds = sbert_model.predict(X_test_sbert)\n",
        "label_map = {0: 'HATE', 1: 'OFFENSIVE', 2: 'NEITHER'}\n",
        "\n",
        "# Construct analysis DataFrame\n",
        "error_df = pd.DataFrame({\n",
        "    'Tweet_Text': df_test['tweet'].values,\n",
        "    'True_Label': [label_map[i] for i in y_test],\n",
        "    'Predicted_Label': [label_map[i] for i in y_test_preds]\n",
        "})\n",
        "\n",
        "# Display cases where Hate Speech was missed\n",
        "print(\"--- ANALYSIS: HATE SPEECH MISCLASSIFIED AS OFFENSIVE ---\")\n",
        "hate_as_offensive = error_df[(error_df['True_Label'] == 'HATE') & (error_df['Predicted_Label'] == 'OFFENSIVE')]\n",
        "print(hate_as_offensive.head(10))\n",
        "\n",
        "# Display cases where Neutral tweets were flagged as Offensive\n",
        "print(\"\\n--- ANALYSIS: NEUTRAL TWEETS MISCLASSIFIED AS OFFENSIVE ---\")\n",
        "neutral_as_offensive = error_df[(error_df['True_Label'] == 'NEITHER') & (error_df['Predicted_Label'] == 'OFFENSIVE')]\n",
        "print(neutral_as_offensive.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfU0Yhmp_cWw",
        "outputId": "b2b490d0-ffdc-44c4-b614-0ceb8b57c412"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Why the model fails on specific cases:**\n",
        "\n",
        "When reviewing the model's errors, there are two clear patterns that explain its limitations.\n",
        "\n",
        "The first problem is with Hate Speech classified as Offensive. All the actual hate cases I reviewed (tweets 1, 34, 40, 62, 77, 82, 97, 102, 111, 153) were predicted as offensive. For example, tweet 111 says \"U coons can't wait for them Oreo J's\" and tweet 34 \"Yea alright redneck white trash\".\n",
        "\n",
        "The model perfectly detects they're aggressive, but fails to take the next step: distinguishing whether it's a generic insult or an attack directed at a specific social group. For SBERT and Random Forest, the difference between \"you're an idiot\" (offensive) and \"you [racial slur]\" (hate) is minimal in vector space. Both have aggressive language, both are personal attacks, but one has an additional layer of discrimination that the model doesn't capture.\n",
        "\n",
        "The second problem is with neutral tweets classified as offensive. Tweet 21 says \"RT @JaeTips I know that's trash\" and tweet 28 \"That's ghetto for super duper! It's...\". Here the problem is informal vocabulary. Words like \"trash\" or \"ghetto\" appear frequently in offensive contexts during training, so the model automatically associates them with aggression.\n",
        "\n",
        "It has no common sense nor understands pragmatics. It can't distinguish between someone saying \"that movie is trash\" (casual opinion) and \"you're trash\" (direct insult). For the model, seeing \"trash\" or \"ghetto\" triggers an alarm, regardless of the broader conversation context.\n",
        "\n",
        "These errors show the fundamental limits of the approach: SBERT captures semantics and syntax, but doesn't understand intention, social context, or the subtleties that differentiate a joke between friends from an actual attack."
      ],
      "metadata": {
        "id": "_R1LFx3NBg30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusions**"
      ],
      "metadata": {
        "id": "g3Vqt9NbTJHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **About the experiment**\n",
        "\n",
        "The difference between Word2Vec and SBERT is substantial. Word2Vec generates static vectors word by word, completely losing the order and structure of the sentence. SBERT captures the complete semantics of the tweet as a unit.\n",
        "\n",
        "The clearest result is that SBERT managed to detect at least some hate cases, something Word2Vec couldn't do at all. This confirms that for complex linguistic phenomena like hate speech, understanding isolated words isn't enough. You need to capture the complete intention of the sentence and how words interact with each other.\n",
        "\n",
        "The model with SBERT is extremely conservative. It achieves perfect precision (1.00) for Hate Speech because it only predicts hate when absolutely certain. The problem is this brutally limits recall: it stays at 0.01. The model doesn't detect implicit or indirect hate, only very obvious cases with direct and explicit insults. Everything else goes to the Offensive category.\n",
        "\n",
        "Data imbalance is the underlying problem. No matter how sophisticated the algorithm (we tested MLP and Random Forest), when 77% of your data is from a single class, the model learns that the safest way to minimize error is to bet on that majority class. The confidence histogram made it clear: the model assigns high probability to Offensive at the slightest doubt. Improving this requires data engineering techniques like oversampling the minority class or obtaining more real examples of hate speech.\n",
        "\n",
        "Linguistic ambiguity is another real limit. Words like \"trash\", \"ghetto\", or \"queer\" constantly cause errors because their meaning depends entirely on context. The model can't distinguish between someone using \"trash\" to say a movie is bad versus using it as a personal insult. It also doesn't understand when a historically offensive term is used in a reclaiming or colloquial way among friends.\n",
        "\n",
        "This confirms that automatic hate detection remains an open problem in NLP. Embeddings capture syntax and semantics, but lack pragmatic understanding or cultural sensitivity. They don't understand intentions, irony, or the social nuances that determine whether something is offensive or hateful in a specific context."
      ],
      "metadata": {
        "id": "M9z98raDB_0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **My Recommendations to go beyond according to the state of the art and literature I've been investigating:**\n",
        "\n",
        "Based on recent literature on hate speech detection, I identify several clear directions to improve our work.\n",
        "\n",
        "**First**, more sophisticated data balancing techniques. SMOTE and its text-specific variants could help us, but according to recent studies, the most effective approach is using data augmentation with generative models. We could use GPT or local LLMs to generate synthetic examples of the minority class while maintaining the linguistic characteristics of real hate. Another option is focal loss or cost-sensitive learning, where we penalize errors in the minority class more heavily during training.\n",
        "\n",
        "**Second**, more recent models than SBERT. Transformers specifically fine-tuned for hate detection outperform general embeddings. HateBERT, ToxicBERT, and similar models were trained specifically on toxic content datasets and better capture nuances. We could also fine-tune models like RoBERTa or DeBERTa directly on our dataset, allowing embeddings to adapt to the particularities of these tweets.\n",
        "\n",
        "**Third**, hybrid architectures that combine multiple information sources. Literature shows good results using contextual embeddings along with explicit features like presence of racial slurs, mentions of protected groups, or specific syntactic patterns. We could extract named entities and verify if they belong to minority groups, or use hate term lexicons as additional features.\n",
        "\n",
        "**Fourth**, multi-task learning approaches. Training the model simultaneously to detect hate and other related tasks like sentiment analysis, toxicity detection, or identification of the attack's target group would allow us to improve general representation. The model would learn shared features that help in all tasks.\n",
        "\n",
        "**Fifth**, explainability and interpretability techniques. Using LIME or SHAP to understand which parts of the tweet are influencing predictions would allow us to identify specific patterns the model uses to distinguish hate from offense. This could reveal biases or features we could explicitly incorporate.\n",
        "\n",
        "**Sixth**, active learning to improve our dataset. Instead of labeling data randomly, the model would identify examples it's most uncertain about and we'd prioritize labeling those. This is especially useful for the minority class where each new example has much more impact.\n",
        "\n",
        "**Seventh**, consider conversational context. Tweets don't exist in a vacuum. Analyzing the complete thread, replies, or user history could give us crucial information to distinguish whether something is contextually offensive or truly hateful. Some recent papers use graph neural networks to capture these relationships.\n",
        "\n",
        "**Eighth**, more sophisticated ensemble methods. Instead of just Random Forest, combining predictions from multiple different models (a fine-tuned transformer, a model with explicit features, one trained with data augmentation) usually improves performance, especially on difficult classes.\n",
        "\n",
        "**My strongest recommendation** would be to start with fine-tuning a model like HateBERT or RoBERTa on our specific dataset, combined with balancing techniques like focal loss or synthetic example generation. That alone should give us a substantial improvement in minority class recall without sacrificing precision as much."
      ],
      "metadata": {
        "id": "JE-Z9H-2Ci6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Selected Academic Bibliography**\n",
        "\n",
        "1. On the Dataset and Imbalance Problem (Baseline)\n",
        ">Davidson, T., Warmsley, D., Macy, M., & Weber, I. (2017). Automated Hate Speech Detection and the Problem of Offensive Language. Proceedings of the 11th International Conference on Web and Social Media (ICWSM).\n",
        "\n",
        "2. On Specialized Models (HateBERT and Transformers)\n",
        "\n",
        ">Caselli, T., Basile, V., Mitrovi\u0107, J., & Granitzer, M. (2021). HateBERT: Retraining BERT for Abusive Language Detection. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH).\n",
        "\n",
        ">Liu, Y., et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.\n",
        "\n",
        "3. On Explainability and Detailed Evaluation\n",
        "> Mathew, B., Saha, P., Yimam, S. M., Biemann, C., Goyal, P., & Mukherjee, A. (2021). HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection. Proceedings of the AAAI Conference on Artificial Intelligence.\n",
        "\n",
        "4. On Data Augmentation and LLMs\n",
        "\n",
        "> Kumar, V., Choudhary, A., & Cho, E. (2020). Data Augmentation using Pre-trained Transformer Models. arXiv preprint arXiv:2003.02245.\n",
        "\n",
        "5. On Multi-task Learning and Context\n",
        "\n",
        "> Pavlopoulos, J., Sorensen, J., Dixon, L., Thain, N., & Androutsopoulos, I. (2020). Toxicity Detection: Does Context Really Matter? Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)."
      ],
      "metadata": {
        "id": "OUlup5ETDiok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "15Pa8REfMdCJ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}